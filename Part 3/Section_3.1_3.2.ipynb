{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Introduction to frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark?\n",
    "\n",
    "- Spark is a framework for distributed processing.\n",
    "- It is a streamlined alternative to Map-Reduce.\n",
    "- Spark applications can be written in Scala, Java, or Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark?\n",
    "\n",
    "Why learn Spark?\n",
    "\n",
    "- Spark enables you to analyze petabytes of data.\n",
    "- Spark is significantly faster than Map-Reduce.\n",
    "- Paradoxically, Spark's API is simpler than the Map-Reduce API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origins\n",
    "\n",
    "- Spark was initially started at UC Berkeley's AMPLab (AMP = Algorithms Machines People) in 2009.\n",
    "- After being open sourced in 2010 under a BSD license, the project was donated in 2013 to the Apache Software Foundation and switched its license to Apache 2.0.\n",
    "- Spark is one of the most active projects in the Apache Software Foundation and one of the most active open source big data projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essense of Spark\n",
    "\n",
    "What is the basic idea of Spark?\n",
    "- Spark takes the Map-Reduce paradigm and changes it in some critical ways:\n",
    "  - Instead of writing single Map-Reduce jobs, a Spark job consists of a series of map and reduce functions.\n",
    "  - Moreover, the intermediate data is kept in memory instead of being written to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Ecosystem\n",
    "\n",
    "<img src='assets/spark_ecosystem.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Since Spark keeps intermediate data in memory to get speed, what does it make us give up? Where's the catch?</summary>\n",
    "1. Spark does a trade-off between memory and performance.\n",
    "<br>\n",
    "2. While Spark jobs are faster, they also consume more memory.\n",
    "<br>\n",
    "3. Spark outshines Map-Reduce in iterative algorithms where the overhead of saving the results of each step to HDFS slows down Map-Reduce.\n",
    "<br>\n",
    "4. For non-iterative algorithms Spark is comparable to Map-Reduce.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Logging\n",
    "\n",
    "Q: How can I make Spark logging less verbose?\n",
    "- By default Spark logs messages at the `INFO` level.\n",
    "- Here are the steps to make it only print out warnings and errors.\n",
    "\n",
    "```sh\n",
    "cd $SPARK_HOME/conf\n",
    "cp log4j.properties.template log4j.properties\n",
    "```\n",
    "\n",
    "- Edit `log4j.properties` and replace `rootCategory=INFO` with `rootCategory=ERROR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Execution\n",
    "\n",
    "<img src='assets/spark_execution.png'>\n",
    "\n",
    "## Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term | Meaning\n",
    "--- |---\n",
    "Driver | Process that contains the Spark Context\n",
    "Executor | Process that executes one or more Spark tasks\n",
    "Master | Process which manages applications across the cluster, e.g., Spark Master\n",
    "Worker | Process which manages executors on a particular worker node, e.g. Spark Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Job\n",
    "\n",
    "Q: Flip a coin 100 times using Python's `random()` function. What\n",
    "fraction of the time do you get heads?\n",
    "\n",
    "- Initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "    .master('local[4]') \\\n",
    "    .appName('spark-lecture') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define and run the Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads = 400\n",
      "tails = 600\n",
      "ratio = 0.4\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "n = 1000\n",
    "\n",
    "\n",
    "heads = (sc.parallelize(xrange(n))\n",
    "    .map(lambda _: random.random())\n",
    "    .filter(lambda r: r <= 0.5)\n",
    "    .count())\n",
    "\n",
    "tails = n - heads\n",
    "ratio = 1. * heads / n\n",
    "\n",
    "print 'heads =', heads\n",
    "print 'tails =', tails\n",
    "print 'ratio =', ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd= sc.parallelize(xrange(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd3=rdd2.filter(lambda r: r <= 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.count() #action : collect, count, mean #tranformation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- `sc.parallelize` creates an RDD.\n",
    "- `map` and `filter` are *transformations*.\n",
    "  - They create new RDDs from existing RDDs.\n",
    "- `count` is an *action* and brings the data from the RDDs back to the driver.\n",
    "\n",
    "## Spark Terminology\n",
    "\n",
    "Term | Meaning\n",
    "--- | ---\n",
    "RDD | *Resilient Distributed Dataset* or a distributed sequence of records\n",
    "Spark Job | Sequence of transformations on data with a final action\n",
    "Transformation | Spark operation that produces an RDD\n",
    "Action | Spark operation that produces a local object\n",
    "Spark Application | Sequence of Spark jobs and other code\n",
    "\n",
    "- A Spark job pushes the data to the cluster, all computation happens on the *executors*, then the result is sent back to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "In this Spark job what is the transformation is what is the action?\n",
    "<br>\n",
    "`sc.parallelize(xrange(10)).filter(lambda x: x % 2 == 0).collect()`\n",
    "</summary>\n",
    "1. `filter` is the transformation.\n",
    "<br>\n",
    "2. `collect` is the action.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda vs. Functions\n",
    "\n",
    "- Instead of `lambda` you can pass in fully defined functions into `map`, `filter`, and other RDD transformations.\n",
    "- Use `lambda` for short functions.\n",
    "- Use `def` for more substantial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Primes\n",
    "\n",
    "Q: Find all the primes less than 100.\n",
    "\n",
    "- Define function to determine if a number is prime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "    factor_min = 2\n",
    "    factor_max = int(number ** 0.5) + 1\n",
    "    for factor in xrange(factor_min, factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use this to filter out non-primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
     ]
    }
   ],
   "source": [
    "numbers = xrange(2, 100)\n",
    "\n",
    "primes = (sc.parallelize(numbers)\n",
    "    .filter(is_prime)\n",
    "    .collect())\n",
    "\n",
    "print primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers=xrange(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd.filter(is_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd2.filter(lambda x: x>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<img src='assets/spark_execution.png'>\n",
    "\n",
    "<details>\n",
    "<summary>Q: Where does `is_prime` execute?</summary>\n",
    "A: On the executors.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: Where does is the RDD object collected?</summary>\n",
    "A: On the driver.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "- Common RDD Constructors\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`sc.parallelize(list)` | Create RDD of elements of list\n",
    "`sc.textFile(path)` | Create RDD of lines from file\n",
    "\n",
    "- Common Transformations\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`filter(lambda x: x % 2 == 0)` | Discard non-even elements\n",
    "`map(lambda x: x * 2)` | Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())` | Split each string into words\n",
    "`flatMap(lambda x: x.split())` | Split each string into words and flatten sequence\n",
    "`sample(withReplacement = True, 0.25)` | Create sample of 25% of elements with replacement\n",
    "`union(rdd)` | Append `rdd` to existing RDD\n",
    "`distinct()` | Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending = False)` | Sort elements in descending order\n",
    "\n",
    "- Common Actions\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`collect()` | Convert RDD to in-memory list \n",
    "`take(3)` | First 3 elements of RDD \n",
    "`top(3)` | Top 3 elements of RDD\n",
    "`takeSample(withReplacement = True, 3)` | Create sample of 3 elements with replacement\n",
    "`sum()` | Find element sum (assumes numeric elements)\n",
    "`mean()` | Find element mean (assumes numeric elements)\n",
    "`stdev()` | Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 5, 2, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 3, 2, 2, 1, 4, 5]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6, 5, 4, 3, 2]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(xrange(2, 10)).sortBy(lambda x: x, ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt').map(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt').flatMap(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map vs. FlatMap\n",
    "\n",
    "Here's the difference between `map` and `flatMap`:\n",
    "\n",
    "- Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'hello', u'world'],\n",
       " [u'another', u'line'],\n",
       " [u'yet', u'another', u'line'],\n",
       " [u'yet', u'another', u'another', u'line']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd= sc.textFile('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'hello', u'world'],\n",
       " [u'another', u'line'],\n",
       " [u'yet', u'another', u'line'],\n",
       " [u'yet', u'another', u'another', u'line']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FlatMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello',\n",
       " u'world',\n",
       " u'another',\n",
       " u'line',\n",
       " u'yet',\n",
       " u'another',\n",
       " u'line',\n",
       " u'yet',\n",
       " u'another',\n",
       " u'another',\n",
       " u'line']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDD\n",
    "\n",
    "At this point we know how to aggregate values across an RDD. If we have an RDD containing sales transactions we can find the total revenue across all transactions.\n",
    "\n",
    "Q: Using the following sales data find the total revenue across all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'106    11/19/2014     202     CA     331        330.00',\n",
       " u'105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt').top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#ID    Date           Store   State  Product    Amount',\n",
       " u'101    11/13/2014     100     WA     331        300.00']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'106', u'11/19/2014', u'202', u'CA', u'331', u'330.00'],\n",
       " [u'105', u'11/19/2014', u'202', u'CA', u'321', u'200.00']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'#ID', u'Date', u'Store', u'State', u'Product', u'Amount']]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: x[0].startswith('#'))\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00'],\n",
       " [u'104', u'11/18/2014', u'700', u'OR', u'329', u'450.00'],\n",
       " [u'102', u'11/15/2014', u'203', u'CA', u'321', u'200.00']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick last field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'300.00', u'450.00', u'200.00']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to float and then sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: float(x[-1]))\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey\n",
    "\n",
    "Q: Calculate revenue per state?\n",
    "\n",
    "- Instead of creating a sequence of revenue numbers we can create tuples of states and revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 300.0),\n",
       " (u'OR', 450.0),\n",
       " (u'CA', 200.0),\n",
       " (u'CA', 330.0),\n",
       " (u'WA', 750.0),\n",
       " (u'CA', 200.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use `reduceByKey` to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CA', 730.0), (u'WA', 1050.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Find the state with the highest total revenue.\n",
    "\n",
    "- You can either use the action `top` or the transformation `sortBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1])\\\n",
    "    .map(lambda x: x[1])\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What does `reduceByKey` do?</summary>\n",
    "1. It is like a reducer.\n",
    "<br>\n",
    "2. If the RDD is made up of key-value pairs, it combines the values across all tuples with the same key by using the function we pass to it.\n",
    "<br>\n",
    "3. It only works on RDDs made up of key-value pairs or 2-tuples.\n",
    "</details>\n",
    "\n",
    "## Notes\n",
    "\n",
    "- `reduceByKey` only works on RDDs made up of 2-tuples.\n",
    "- `reduceByKey` works as both a reducer and a combiner.\n",
    "- It requires that the operation is associative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "Q: Implement word count in Spark.\n",
    "\n",
    "- Create some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'line', 3), (u'another', 4), (u'world', 1), (u'hello', 1), (u'yet', 2)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2= rdd.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hello', 1),\n",
       " (u'world', 1),\n",
       " (u'another', 1),\n",
       " (u'line', 1),\n",
       " (u'yet', 1),\n",
       " (u'another', 1),\n",
       " (u'line', 1),\n",
       " (u'yet', 1),\n",
       " (u'another', 1),\n",
       " (u'another', 1),\n",
       " (u'line', 1)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'line', 3), (u'another', 4), (u'world', 1), (u'hello', 1), (u'yet', 2)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.reduceByKey(lambda a, b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making List Indexing Readable\n",
    "\n",
    "- While this code looks reasonable, the list indexes are cryptic and hard to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1], ascending = False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can make this more readable using Python's argument unpacking feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Unpacking\n",
    "\n",
    "Q: Which version of `getCity` is more readable and why?\n",
    "\n",
    "- Consider this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getCity1(client) = SF\n",
      "getCity1(client) = SF\n"
     ]
    }
   ],
   "source": [
    "client = ('Dmitri', 'Smith', 'SF')\n",
    "\n",
    "def getCity1(client):\n",
    "    return client[2]\n",
    "\n",
    "def getCity2((first, last, city)):\n",
    "    return city\n",
    "\n",
    "print 'getCity1(client) =', getCity1(client)\n",
    "print 'getCity1(client) =', getCity2(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between `getCity1` and `getCity2`?\n",
    "- Which is more readable?\n",
    "- What is the essence of argument unpacking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Can argument unpacking work for deeper nested structures?</summary>\n",
    "A: Yes. It can work for arbitrarily nested tuples and lists.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: How would you write `getCity` given\n",
    "<br>\n",
    "`client = ('Dmitri','Smith', ('123 Eddy','SF','CA'))`\n",
    "</summary>\n",
    "`def getCity((first, last, (street, city, state))): return city`\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = ('Dmitri', 'Smith',('123 Eddy', 'SF', 'CA'))\n",
    "\n",
    "def getCity((first, last, (street, city, state))):\n",
    "    return city\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever you find yourself indexing into a tuple consider usingargument unpacking to make it more readable.\n",
    "- Here is what `getCity` looks like without tuple indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def badGetCity(client):\n",
    "    return client[2][1]\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Unpacking In Spark\n",
    "\n",
    "Q: Rewrite the last Spark job using argument unpacking.\n",
    "\n",
    "- Here is the original version of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount: state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code with argument unpacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda (id, date, store, state, product, amount): (state, float(amount)))\\\n",
    "    .reduceByKey(lambda amount1, amount2: amount1 + amount2)\\\n",
    "    .sortBy(lambda (state, amount): amount, ascending = False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case because we have a long list or tuple argument unpacking is a judgement call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupByKey\n",
    "\n",
    "`reduceByKey` lets us aggregate values using sum, max, min, and other associative operations. But what about non-associative operations like average? How can we calculate them?\n",
    "\n",
    "There are several ways to do this.\n",
    "- The first approach is to change the RDD tuples so that the operation becomes associative. \n",
    "  - Instead of `(state, amount)` use `(state, (amount, count))`.\n",
    "- The second approach is to use `groupByKey`, which is like `reduceByKey` except it gathers together all the values in an iterator. \n",
    "  - The iterator can then be reduced in a `map` step immediately after the `groupByKey`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Calculate the average sales per state.\n",
    "\n",
    "- Approach 1: Restructure the tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CA', 243.33333333333334), (u'WA', 525.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], (float(x[-1]), 1)))\\\n",
    "    .reduceByKey(lambda (amount1, count1), (amount2, count2):\\\n",
    "        (amount1 + amount2, count1 + count2))\\\n",
    "    .map(lambda (state, (amount, count)): (state, amount / count))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the argument unpacking we are doing in `reduceByKey` to name the elements of the tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approach 2: Use `groupByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CA', 243.33333333333334), (u'WA', 525.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(iterator):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iterator:\n",
    "        total += x; count += 1\n",
    "    return total / count\n",
    "\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3], float(x[-1])))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda (state, iterator): (state, mean(iterator)))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are using unpacking again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What would be the disadvantage of not using unpacking?</summary>\n",
    "1. We will need to drill down into the elements.\n",
    "<br>\n",
    "2. The code will be harder to read.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What are the pros and cons of `reduceByKey` vs. `groupByKey`?</summary>\n",
    "1. `groupByKey` stores the values for particular key as an iterable.\n",
    "<br>\n",
    "2. This will take up space in memory or on disk.\n",
    "<br>\n",
    "3. `reduceByKey` therefore is more scalable.\n",
    "<br>\n",
    "4. However, `groupByKey` does not require associative reducer operation.\n",
    "<br>\n",
    "5. For this reason `groupByKey` can be easier to program with.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Q: Given a table of employees and locations find the cities that the employees live in.\n",
    "\n",
    "- The easiest way to do this is with a `join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, ('Dee', None)),\n",
       " (14, ('Alice', 'SF')),\n",
       " (14, ('Chad', 'SF')),\n",
       " (15, ('Bob', 'Seattle')),\n",
       " (15, ('Jen', 'Seattle'))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Employees: emp_id, loc_id, name\n",
    "employee_data = [\n",
    "    (101, 14, 'Alice'),\n",
    "    (102, 15, 'Bob'),\n",
    "    (103, 14, 'Chad'),\n",
    "    (104, 15, 'Jen'),\n",
    "    (105, 13, 'Dee') ]\n",
    "\n",
    "# Locations: loc_id, location\n",
    "location_data = [\n",
    "    (14, 'SF'),\n",
    "    (15, 'Seattle'),\n",
    "    (16, 'Portland')]\n",
    "\n",
    "employees = sc.parallelize(employee_data)\n",
    "locations = sc.parallelize(location_data)\n",
    "\n",
    "# Re-key employee records with loc_id\n",
    "employees2 = employees.map(lambda (emp_id, loc_id, name): (loc_id, name))\n",
    "\n",
    "# Now join.\n",
    "employees2.leftOuterJoin(locations).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: How can we keep employees that don't have a valid location ID in the final result?</summary>\n",
    "1. Use `leftOuterJoin` to keep employees without location IDs.\n",
    "<br>\n",
    "2. Use `rightOuterJoin` to keep locations without employees. \n",
    "<br>\n",
    "3. Use `fullOuterJoin` to keep both.\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Statistics\n",
    "\n",
    "Q: How would you calculate the mean, variance, and standard deviation of a sample produced by Python's `random` function?\n",
    "\n",
    "- Create an RDD and apply the statistical actions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean     = 0.506333143283\n",
      "variance = 0.0813565893047\n",
      "stdev    = 0.285230765004\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "\n",
    "list = [random.random() for _ in xrange(n)]\n",
    "\n",
    "rdd = sc.parallelize(list)\n",
    "\n",
    "print 'mean     =', rdd.mean()\n",
    "print 'variance =', rdd.variance()\n",
    "print 'stdev    =', rdd.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What requirement does an RDD have to satisfy before you can apply these statistical actions to it? \n",
    "</summary>\n",
    "The RDD must consist of numeric elements.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is the advantage of using Spark vs Numpy to calculate mean or standard deviation?</summary>\n",
    "The calculation is distributed across different machines and will be more scalable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Laziness\n",
    "\n",
    "- Q: What is this Spark job doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 5.38 ms, total: 18.1 ms\n",
      "Wall time: 1.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000000\n",
    "\n",
    "%time sc.parallelize(xrange(n)).map(lambda x: x + 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: How is the following job different from the previous one? How\n",
    "  long do you expect it to take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 ms, sys: 1.84 ms, total: 4.36 ms\n",
      "Wall time: 7.46 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[210] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sc.parallelize(xrange(n)).map(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Why did the second job complete so much faster?</summary>\n",
    "1. Because Spark is lazy. \n",
    "<br>\n",
    "2. Transformations produce new RDDs and do no operations on the data.\n",
    "<br>\n",
    "3. Nothing happens until an action is applied to an RDD.\n",
    "<br>\n",
    "4. An RDD is the *recipe* for a transformation, rather than the *result* of the transformation.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is the benefit of keeping the recipe instead of the result of the action?</summary>\n",
    "1. It saves memory.\n",
    "<br>\n",
    "2. It produces *resilience*. \n",
    "<br>\n",
    "3. If an RDD loses data on a machine, it always knows how to recompute it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "Besides reading data Spark and also write data out to a file system.\n",
    "\n",
    "Q: Calculate the squares of integers from 1 to 10 and write them out to `squares.txt`.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(xrange(10))\n",
    "rdd2 = rdd1.map(lambda x: x ** 2)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: squares.txt: Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the output is a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  0 Jun  8 22:13 _SUCCESS\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  4 Jun  8 22:13 part-00000\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  7 Jun  8 22:13 part-00001\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00002\r\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  9 Jun  8 22:13 part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squares.txt/part-00000\r\n",
      "0\r\n",
      "1\r\n",
      "squares.txt/part-00001\r\n",
      "4\r\n",
      "9\r\n",
      "16\r\n",
      "squares.txt/part-00002\r\n",
      "25\r\n",
      "36\r\n",
      "squares.txt/part-00003\r\n",
      "49\r\n",
      "64\r\n",
      "81\r\n"
     ]
    }
   ],
   "source": [
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: What's going on? Why are there four files (excluding `_SUCCESS`) in the output directory?</summary>\n",
    "1. There were four threads that were processing the RDD.\n",
    "<br>\n",
    "2. The RDD was split up in four partitions (default with local mode: number of cores on the local machine).\n",
    "<br>\n",
    "3. Each partition was processed in a different task.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Q: Can we control the number of partitions/tasks that Spark uses for processing data? Solve the same problem as above but this time with 5 tasks.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 5\n",
    "rdd1 = sc.parallelize(xrange(10), partitions)\n",
    "rdd2 = rdd1.map(lambda x: x ** 2)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  0 Jun  8 22:13 _SUCCESS\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  4 Jun  8 22:13 part-00000\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  4 Jun  8 22:13 part-00001\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00002\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00003\n",
      "-rw-r--r--  1 koyuki.nakamori  staff  6 Jun  8 22:13 part-00004\n",
      "squares.txt/part-00000\n",
      "0\n",
      "1\n",
      "squares.txt/part-00001\n",
      "4\n",
      "9\n",
      "squares.txt/part-00002\n",
      "16\n",
      "25\n",
      "squares.txt/part-00003\n",
      "36\n",
      "49\n",
      "squares.txt/part-00004\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "!ls -l squares.txt\n",
    "\n",
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: How many partitions does Spark use by default?</summary>\n",
    "1. For operations like parallelize, it depends on the cluster manager:\n",
    "<br>\n",
    "  - Local mode: number of cores on the local machine\n",
    "<br>\n",
    "  - Others: total number of cores on all executor nodes or 2, whichever is larger\n",
    "<br>\n",
    "2. If you read an HDFS file into an RDD, Spark uses one partition per block.\n",
    "<br>\n",
    "3. If you read a file into an RDD from S3 or some other source, Spark uses 1 partition per 32 MB of data.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: If I read a file that is 200 MB into an RDD, how many partitions will that have?</summary>\n",
    "1. If the file is on HDFS that will produce 2 partitions (each is 128 MB).\n",
    "<br>\n",
    "2. If the file is on S3 or some other file system it will produce 7 partitions.\n",
    "<br>\n",
    "3. You can also control the number of partitions by passing in an additional argument into `textFile`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Terminology\n",
    "\n",
    "<img src='assets/spark_execution.png'>\n",
    "\n",
    "Term | Meaning\n",
    "--- |---\n",
    "Task | Single thread in an executor\n",
    "Partition | Data processed by a single task\n",
    "Record | Records make up a partition that is processed by a single task\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Every Spark application gets executors when you create a new `SparkContext`.\n",
    "- You can specify how many cores to assign to each executor.\n",
    "- A core is equivalent to a thread.\n",
    "- The number of cores determine how many tasks can run concurrently on an executor.\n",
    "- Each task corresponds to one partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Q: Suppose you have 2 executors, each with 2 cores--so a total of 4\n",
    "cores. And you start a Spark job with 8 partitions. How many tasks\n",
    "will run concurrently?\n",
    "</summary>\n",
    "4 tasks will execute concurrently.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What happens to the other partitions?</summary>\n",
    "1. The other partitions wait in queue until a task thread becomes available.\n",
    "<br>\n",
    "2. Think of cores as turnstile gates at a train station, and partitions as people.\n",
    "<br>\n",
    "3. The number of turnstiles determine how many people can get through at once.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: How many Spark jobs can you have in a Spark application?</summary>\n",
    "As many as you want.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: How many Spark applications and Spark jobs are in this IPython Notebook?</summary>\n",
    "1. There is one Spark application because there is one `SparkContext`.\n",
    "<br>\n",
    "2. There are as many Spark jobs as we have invoked actions on RDDs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Quotes\n",
    "\n",
    "Q: Find the date on which AAPL's stock price was the highest.\n",
    "\n",
    "Suppose you have stock market data from Yahoo! for AAPL from <http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices>. The data is in CSV format and has these values.\n",
    "\n",
    "Date | Open | High | Low | Close | Volume | Adj Close\n",
    "---- | --- | --- | --- | -----   |------ | ---\n",
    "11-18-2014 | 113.94 | 115.69 | 113.89 | 115.47 | 44,200,30 | 115.47\n",
    "11-17-2014 | 114.27 | 117.28 | 113.30 | 113.99 | 46,746,700 | 113.99\n",
    "\n",
    "Here is what the CSV looks like:\n",
    "    \n",
    "    csv = [\n",
    "      \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "      \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "      \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "    ]\n",
    "\n",
    "Lets find the date on which the price was the highest. \n",
    "\n",
    "<details>\n",
    "<summary>Q: What two fields do we need to extract?</summary>\n",
    "1. *Date* and *Adj Close*.\n",
    "<br>\n",
    "2. We want to use *Adj Close* instead of *High* so our calculation is not affected by stock splits.\n",
    "</details>\n",
    "\n",
    "<details><summary>Q: What field should we sort on?</summary>\n",
    "*Adj Close*\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Q: What sequence of operations would we need to perform?</summary>\n",
    "1. Use `filter` to remove the header line.\n",
    "<br>\n",
    "2. Use `map` to split each row into fields.\n",
    "<br>\n",
    "3. Use `map` to extract *Adj Close* and *Date*.\n",
    "<br>\n",
    "4. Use `sortBy` to sort descending on *Adj Close*.\n",
    "<br>\n",
    "5. Use `take(1)` to get the highest value.\n",
    "</details>\n",
    "\n",
    "- Here is full source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(115.47, '2014-11-18')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = [\n",
    "  \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "  \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "  \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "]\n",
    "\n",
    "sc.parallelize(csv) \\\n",
    "  .filter(lambda line: not line.startswith(\"#\")) \\\n",
    "  .map(lambda line: line.split(\",\")) \\\n",
    "  .map(lambda fields: (float(fields[-1]), fields[0])) \\\n",
    "  .sortBy(lambda (close, date): close, ascending = False) \\\n",
    "  .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the program for finding the high of any stock that stores\n",
    "  the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "# import re\n",
    "\n",
    "# def get_stock_high(symbol):\n",
    "    \n",
    "#     url = 'http://real-chart.finance.yahoo.com/table.csv?s=' + symbol + '&g=d&ignore=.csv'\n",
    "#     csv = urllib2.urlopen(url).read()\n",
    "#     csv_lines = csv.split('\\n')\n",
    "\n",
    "#     #print csv_lines\n",
    "#     stock_rdd = sc.parallelize(csv_lines)\\\n",
    "#         .filter(lambda line: re.match(r'\\d', line))\\\n",
    "#         .map(lambda line: line.split(\",\"))\\\n",
    "#         .map(lambda fields: (float(fields[-1]), fields[0]))\\\n",
    "#         .sortBy(lambda (close, date): close, ascending = False)\n",
    "\n",
    "#     return stock_rdd.take(1)\n",
    "\n",
    "# get_stock_high('AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Spark is high-level like Hive and Pig.\n",
    "- At the same time it does not invent a new language.\n",
    "- This allows it to leverage the ecosystem of tools that Python, Scala, and Java provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Caching\n",
    "\n",
    "- Consider this Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 500000\n",
    "numbers = [random.random() for _ in xrange(n)]\n",
    "rdd1 = sc.parallelize(numbers)\n",
    "rdd2 = rdd1.sortBy(lambda number: number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets time running `count()` on `rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use `cache()` to make Spark cache the RDD.\n",
    "\n",
    "- Let's cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.19 ms, sys: 2.38 ms, total: 8.58 ms\n",
      "Wall time: 147 ms\n",
      "CPU times: user 4.99 ms, sys: 1.8 ms, total: 6.79 ms\n",
      "Wall time: 45.5 ms\n",
      "CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.18 ms\n",
      "Wall time: 53.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Calling `cache()` flips a flag on the RDD. \n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using `unpersist()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Will `unpersist` uncache the RDD immediately or does it wait for an action?</summary>\n",
    "A: It unpersists immediately.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and Persistence\n",
    "\n",
    "Q: Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "- Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[190] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "\n",
    "<details>\n",
    "<summary>Q: Will the RDD be stored on disk at this point?</summary>\n",
    "A: No. It will get stored after we call an action.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence Levels\n",
    "\n",
    "Level | Meaning\n",
    "--- | ---\n",
    "`MEMORY_ONLY` | Same as `cache()`\n",
    "`MEMORY_AND_DISK` | Cache in memory then overflow to disk\n",
    "`MEMORY_AND_DISK_SER` | Like above; in cache keep objects serialized instead of live \n",
    "`DISK_ONLY` | Cache to disk not to memory\n",
    "\n",
    "### Notes\n",
    "\n",
    "- `MEMORY_AND_DISK_SER` is a good compromise between the levels. \n",
    "- Fast, but not too expensive.\n",
    "- Make sure you unpersist when you don't need the RDD any more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLlib\n",
    "\n",
    "- MLlib is Sparks machine learning (ML) library.\n",
    "- Its goal is to make practical machine learning scalable and easy.\n",
    "- It consists of common learning algorithms, including:\n",
    "  - Classification/Regression\n",
    "    - Logistic Regression, Support vector machine (SVM), Naive Bayes, Gradient Boosted Trees, Random Forests, Multilayer Perceptron (e.g., a neural network), Generalized linear regression (GLM)\n",
    "  - Recommenders/Collaborative Filtering\n",
    "    - Non-negative matrix factorization (NMF)\n",
    "  - Decomposition\n",
    "    - Singular value decomposition (SVD), (Principal component analysis)\n",
    "  - Clustering\n",
    "    - K-Means, Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "\n",
    "- *\"s3:\" URLs break when Secret Key contains a slash, even if encoded* <https://issues.apache.org/jira/browse/HADOOP-3733>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
